{
  "agent_id": "coder1",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_cs.LG_2508.21040v1_FW_GAN_Frequency_Driven_Handwriting_Synthesis_wit",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on cs.LG_2508.21040v1_FW-GAN-Frequency-Driven-Handwriting-Synthesis-wit with content analysis. Detected project type: computer vision (confidence score: 10 matches).",
    "key_algorithms": [
      "Thesis",
      "Offline",
      "Variational",
      "Efficient",
      "Compact",
      "Fw-Gan",
      "Handwriting",
      "Wave-Mlp",
      "Both",
      "Cal"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "PDF: cs.LG_2508.21040v1_FW-GAN-Frequency-Driven-Handwriting-Synthesis-wit.pdf\nChunk: 1/1\n==================================================\n\n--- Page 1 ---\nFW-GAN: Frequency-Driven Handwriting Synthesis\nwith Wave-Modulated MLP Generator\nHuynh Tong Dang Khoaa,b, Dang Hoai Nama,b, Vo Nguyen Le Duya,b,\u2217\naUniversity of Information Technology, Ho Chi Minh City, Vietnam\nbVietnam National University, Ho Chi Minh City, Vietnam\nAbstract\nLabeled handwriting data is often scarce, limiting the effectiveness of recog-\nnition systems that require diverse, style-consistent training samples. Hand-\nwriting synthesis offers a promising solution by generating artificial data\nto augment training. However, current methods face two major limitations.\nFirst,mostarebuiltonconventionalconvolutionalarchitectures,whichstrug-\ngle to model long-range dependencies and complex stroke patterns. Second,\ntheylargelyignorethecrucialroleoffrequencyinformation, whichisessential\nfor capturing fine-grained stylistic and structural details in handwriting. To\naddress these challenges, we propose FW-GAN, a one-shot handwriting syn-\nthesis framework that generates realistic, writer-consistent text from a single\nexample. Our generator integrates a phase-aware Wave-MLP to better cap-\nturespatialrelationshipswhilepreservingsubtlestylisticcues. Wefurtherin-\ntroduce a frequency-guided discriminator that leverages high-frequency com-\nponents to enhance the authenticity detection of generated samples. Addi-\ntionally, we introduce a novel Frequency Distribution Loss that aligns the\nfrequency characteristics of synthetic and real handwriting, thereby enhanc-\ning visual fidelity. Experiments on Vietnamese and English handwriting\ndatasets demonstrate that FW-GAN generates high-quality, style-consistent\nhandwriting, making it a valuable tool for augmenting data in low-resource\nhandwriting recognition (HTR) pipelines. Official implementation is avail-\nable at https://github.com/DAIR-Group/FW-GAN\nKeywords: Handwritten text synthesis, Wavelet transform, One-shot\nlearning, Vietnamese handwriting, Synthetic data\n\u2217Corresponding author. E-mail: duyvnl@uit.edu.vn\nPreprint submitted to arXiv August 29, 2025arXiv:2508.21040v1  [cs.CV]  28 Aug 2025\n\n--- Page 2 ---\n1. Introduction\nDespite rapid technological progress, handwritten documents continue to\nplay an important role across a wide range of real-world applications be-\ncause they often capture information in contexts where digital input is im-\npractical or unavailable. In government institutions, handwritten forms are\nstill widely used for signatures, annotations, and quick data collection, espe-\ncially in low-digital or high-security environments. In cultural heritage and\nhistorical research, handwritten manuscripts, letters, and archival materials\nserve as primary records of linguistic, social, and political history, requiring\nprecise preservation and digitization for scholarly study. Similarly, in edu-\ncation, handwritten exams and assignments remain the norm for evaluating\nstudents\u2019 knowledge, creativity, and problem-solving skills in a natural and\nflexible medium. Beyond formal systems, handwriting is deeply tied to indi-\nvidual identity and expression, making it not only a functional tool but also\na carrier of personality and nuance that typed text cannot fully convey. As\na result, robust handwriting recognition (HTR) and synthesis technologies\nare essential to bridge analog and digital worlds in these domains. However,\nHTR systems face persistent challenges due to high variability in writing\nstyles, differences in language structures, and varying image quality caused\nby scanning or photographic conditions [1, 2, 3, 4]. These challenges are\nfurther amplified in low-resource languages, where acquiring large annotated\ndatasets for training is costly, labor-intensive, or entirely impractical.\nRecent advances in deep learning, especially Transformer-based models,\nhave yielded impressive performance gains in HTR. Yet, these models are\nheavily data-dependent and often fail to generalize well in data-scarce sce-\nnarios. Handwriting synthesis (HS) has emerged as a promising solution, of-\nferingautomatedgenerationofartificialsamplestoaugmenttrainingdatasets\n[5, 6]. Effective HS systems aim to generate lexically correct and stylistically\nconsistent handwriting for arbitrary textual inputs. Achieving this, however,\nremains difficult due to the need to capture subtle visual cues such as stroke\ncurvature, spacing irregularities, and writer-specific traits, especially when\nlimited samples are available. These challenges are even more pronounced in\ncomplex scripts like Vietnamese, where tone marks and diacritic placement\nintroduce additional orthographic variation.\nWhile recent generative models, particularly those based on Generative\n2\n\n--- Page 3 ---\nAdversarial Networks (GANs), have improved synthesis realism, two core\nlimitations still hinder performance. First, most existing approaches over-\nlook the critical importance of frequency-domain information in handwriting\nsynthesis. The F-Principle [7] reveals that neural networks tend to learn\nlow-frequency components first, leading to synthetic images that may pre-\nserve global structure but lack high-frequency details such as sharp stroke\nedges and pen pressure variations, which are essential for realistic handwrit-\ning. Second, many prior models adopt a CNN-based generator, which is\ninherently constrained by their limited receptive field. These convolutional\nstructures emphasize local spatial features but fail to capture long-range de-\npendencies and global frequency characteristics, both crucial for producing\ncoherent and style-consistent handwriting across longer text sequences.\nTo address these challenges, we introduce FW-GAN, a frequency-aware,\none-shot handwriting synthesis framework designed to produce realistic and\nwriter-consistent handwritten text from a single reference. Our method en-\nhances the generator with phase-aware Wave-MLP modules [8], which dy-\nnamicallyaggregatetokens, improvingthesynthesisofthemodel. Wefurther\nintroduce a novel Frequency Distribution Loss [9], which explicitly supervises\nthe generator to match the frequency characteristics of authentic handwrit-\ning. Additionally, we propose a high-frequency discriminator for detecting\nsubtle artifacts and guiding the generator toward more faithful synthesis.\nExperiments on both English and Vietnamese handwriting datasets have ver-\nified the excellent performance of the proposed model. Its ability to augment\nlow-resource training data positions it as a practical solution for enhancing\nHTR systems used in real-world expert applications. The main contributions\nto this paper are as follows:\n\u2022We integrate Phase-Aware Wave-MLP modules that dynamically ag-\ngregate spatial tokens through trigonometric transformations, enabling\nbetter modeling of stylistic patterns while maintaining computational\nefficiency.\n\u2022We introduce a novel loss function that explicitly aligns the spectral\ncharacteristics between real and synthetic handwriting, ensuring gener-\nated samples preserve fine-grained frequency details essential for visual\nauthenticity.\n\u2022We propose a frequency-guided discriminator alongside the standard\nspatial discriminator, where the high-frequency discriminator leverages\n3\n\n--- Page 4 ---\nwavelet decomposition to detect subtle artifacts that the spatial dis-\ncriminator might miss, providing complementary adversarial supervi-\nsion.\n\u2022ExtensiveexperimentsonEnglishandVietnamesehandwrittendatasets\ndemonstrate that FW-GAN significantly outperforms the state-of-the-\nart.\n2. Related Work\nHandwriting text generation (HTG) encompasses a wide range of tech-\nniques aimed at producing synthetic handwritten content that resembles hu-\nman writing in both content and style. These techniques can be broadly\ndivided into two categories based on how handwriting is represented: on-\nline methods, which model the dynamic pen trajectory, and offline methods,\nwhich treat handwriting as a static visual artifact. Each paradigm offers\nunique strengths and faces specific limitations.\n2.1. Online Handwriting Generation\nOnline HTG methods generate handwriting by simulating the pen\u2019s mo-\ntion over time. These techniques employ sequence-based models such as\nLSTMs[5],conditionalvariationalrecurrentnetworks[10],ortemporalCNNs[11]\nto predict a series of pen coordinates conditioned on the target text. The\nfoundational work by Graves [5] introduced this paradigm but did not in-\ncorporate style conditioning. Subsequent studies [10, 11, 12] addressed this\ngap by extracting style representations from exemplar images and integrating\nthem into the sequence model.\nGenerative Adversarial Networks (GANs) have also been adapted for this\nmodality - for instance, Ji et al. [13] incorporated a discriminator to enhance\npen trajectory realism. However, despite these advances, online methods\nface two fundamental obstacles. First, modeling long-range dependencies in\npen motion remains difficult. Second, online data requires access to temporal\nstrokerecordings, whichareexpensivetocollectandunavailableforhistorical\nmanuscripts. As a result, many recent works, including ours, have shifted\nfocus toward the offline generation setting, where broader datasets and real-\nworld applications are more accessible.\n4\n\n--- Page 5 ---\n2.2. Offline Handwriting Generation\nOffline HTG synthesizes handwriting directly in static images. Earlier\nefforts [14, 15, 16, 17] relied on manually segmented glyphs and heuristics\nfor layout, ligature connections, and background blending. These approaches\nrequired substantial manual design and could not generalize to unseen char-\nacters or styles. Learning-based methods, particularly those using GANs,\nhave largely replaced these hand-crafted pipelines [18, 19, 20, 21]. Alonso et\nal.[18] introduced a GAN-based model that generated fixed-size word images\nfrom text embeddings. This was extended in ScrabbleGAN[19] to variable-\nlength synthesis using a patch-based character assembly approach. Subse-\nquent works [20, 21, 22] have adopted similar strategies, often conditioning\ngeneration on text encoded as character-level one-hot vectors. Other ap-\nproaches [23, 24] perform direct image-to-image style transfer using typeface-\nrendered input images. Furthermore, style conditioning in modern offline\nHTGmodelsvariesingranularity,rangingfromparagraph-levelreferences[25]\nto word-level or even single-word exemplars [21, 23]. In general, richer style\ninputs lead to better synthesis fidelity [24]. Despite their strong visual per-\nformance, most existing methods rely on convolutional backbones derived\nfrom BigGAN [26], which are limited in capturing global structural patterns\ndue to their restricted receptive fields. As a result, these models tend to fo-\ncus heavily on local features, which hinders their ability to learn the overall\nhandwriting structure and may lead to suboptimal style representation.\nHowever, combining CNNs and Transformers to jointly model global\nand local dependencies introduces significant computational overhead [6, 27],\nwhich is particularly problematic due to the extensive training requirements\nof GANs. In the computer vision field, while several works have explored\nhybrid architectures that integrate Transformers with CNNs or MLPs [28,\n29, 30, 6, 27], these models often suffer from high complexity due to their\nlarge number of parameters. On the other hand, recent progress in MLP-\nbased networks [31, 8, 32, 33] shows that they can achieve competitive per-\nformance compared to CNNs while being more lightweight, making them an\nattractive option for efficient generation. Motivated by this, we explore the\ncombination of CNN and MLP modules in the generation process, aiming\nto develop a handwriting generator that balances high-quality output with\ncomputational efficiency.\nIn addition to architectural limitations, many existing approaches over-\nlookfrequency-domaincharacteristics. TheFrequencyPrinciple(F-Principle)[7]\nsuggests that during training, neural networks tend to learn low-frequency\n5\n\n--- Page 6 ---\n(smooth) components of the target function before fitting high-frequency\n(detailed) components. This principle has been theoretically supported in\nvarious contexts, including infinite data regimes [34], wide neural networks\nin the Neural Tangent Kernel (NTK) regime [35], and finite sample set-\ntings [36, 37, 38, 39, 40, 41]. Additionally, E et al. [42] demonstrate that this\nbehavior can naturally arise from the integral formulation of network train-\ning. Based on this insight, analyzing the frequency components of real and\ngenerated handwriting images may help uncover key differences and guide\nfurther improvements in model design.\nBuilding upon these insights, we enhance the generator by integrating\nWave-MLP modules into the convolutional blocks of the BigGAN backbone,\ncommonly adopted in several state-of-the-art methods [21, 43, 6, 27]. These\nmodules dynamically aggregate spatial information through trigonometric\ntransformations, improving synthesis quality while maintaining a compact\nmodel size. To further enhance realism, we introduce a Frequency Distri-\nbution Loss (FDL) that aligns the spectral statistics of generated handwrit-\ning with those of real samples. Additionally, we deploy a frequency-guided\ndiscriminator alongside the standard spatial discriminator, where the high-\nfrequency discriminator leverages wavelet decomposition to detect subtle ar-\ntifacts that the spatial discriminator might miss, providing complementary\nadversarial supervision across both spatial and frequency domains. We adopt\nthe framework similar to HiGAN [21] as a practical baseline due to its sta-\nble training behavior and compatibility with a BigGAN-style generator to\ndevelop our proposed methods. However, our proposed architectural modi-\nfications and training objectives are independent of this framework and can\nbe readily incorporated into alternative generative models. An overview of\nthe proposed framework is illustrated in Figure 1.\n3. Proposed Approach\n3.1. Problem Formulation\nGiven a single handwritten word image xauthored by a target writer w,\nwhich has a calligraphic style z, our goal is to synthesize new handwritten\nimages that preserve the unique calligraphic style of writer w. We define a\nquery set A={ak}Q\nk=1, where each akis a word of arbitrary length composed\nfromageneralcharacterset. Eachqueryword akisrepresentedasacharacter\nsequence ak= [ak,1, . . . , a k,Lk], where Lkis the length of the k-th word. To\n6\n\n--- Page 7 ---\nFigure 1: Overview of the proposed FW-GAN\ngenerate a corresponding handwritten image \u02c6xkin the style of x, we train a\ngenerator Gthat maps a character sequence and a style vector to an image:\n\u02c6xk=G(ak,z). (1)\nThere are two ways of obtaining the style vector z: it can either be\nsampled from a standard normal distribution N(0,I)or extracted directly\nfrom the reference image xusing a style encoder Ebyz=E(x). This\nflexibility enables the generator to reproduce the specific handwriting style\nof the target writer.\n3.2. Overall Architecture\nOur model features a hierarchical generator composed of WaveGBlocks,\nwhichutilizephase-awareWave-MLPlayerstoeffectivelycapturespatialpat-\nterns and preserve the sequential structure of character layouts, supporting\nthe generation of variable-length handwritten text. The generator is condi-\ntioned on both character-level content embeddings and writer-specific style\n7\n\n--- Page 8 ---\nFigure 2: Overview of the proposed WaveGBlock\nfeatures, enabling personalized and flexible synthesis. To enhance realism\nand preserve structural coherence, we deploy a dual discriminator architec-\nture where a standard spatial discriminator and a frequency-guided discrim-\ninator work in parallel, with the latter leveraging high-frequency wavelet\ncomponents to detect subtle artifacts that spatial analysis might miss. The\nsystem also includes a recognizer to enforce content accuracy, a style encoder\nto extract personalized writing styles, and a writer identifier to maintain\nwriter-specific characteristics during training. All components are trained\ntogether in a unified framework, where the generator learns to produce hand-\nwriting that is not only legible and accurate but also visually consistent with\nthe target style while maintaining authentic frequency characteristics.\n3.2.1. Hierarchical Generator with Wave-MLP\nOur handwriting synthesis framework employs a hierarchical generator,\njointly conditioned on a writer-specific latent style vector z\u2208Rdand a sym-\nbolic character sequence y\u2208 {0,1}n\u00d7L, where nis the vocabulary size and L\n8\n\n--- Page 9 ---\nis the sequence length. The style vector zis divided into multiple segments:\nthe first segment is fused with the character input y(via elementwise mul-\ntiplication) to form a unified latent representation. This representation is\nthen projected and reshaped to initialize the feature map F0\u2208RC0\u00d7H0\u00d7W0.\nThe remaining segments of zare gradually injected into subsequent Wave-\nModulated Generator\u2019s Blocks (WaveGBlocks) using conditional batch nor-\nmalization, allowing the style information to guide feature transformations at\neach stage. The feature map is progressively upsampled and refined through\nthe hierarchy of WaveGBlocks, ultimately producing a variable-width hand-\nwriting image that accurately reflects both the content and the writer\u2019s style.\nEach WaveGBlock introduces a new mechanism, Wave-MLP for feature\ninteraction, inspired by wave-based representations in quantum mechanics.\nIn this framework, entities such as electrons and photons are described by\nwave functions containing both amplitude and phase components (e.g., de\nBroglie waves). Drawing from this idea, we define tokens as spatial units in\nthe intermediate feature maps, analogous to tokens in sequence models but\nrepresenting local image regions. These tokens are modeled as waves with\namplitude and phase that can dynamically interact. The amplitude repre-\nsents the token\u2019s real-valued content, while the phase encodes its relational\nrole and direction, forming a complex-valued signal through interaction with\nfixed MLP weights. Wave-MLP incorporates a Phase-Aware Token Mixing\n(PATM) module to dynamically estimate the phase for each token, as se-\nmantic meaning can vary with input. In Wave-MLP, token is represented as\na Wave~ fjwith amplitude and phase information:\n\u02dcfj=|fj| \u2299ei\u03b8j, j= 1,2,3, . . . , n, (2)\nwhere |fj|is the real-valued amplitude, \u03b8jis the learned phase parameter,\nand\u2299denotes elementwise multiplication.\nThe Wave-MLP framework, illustrated in Figure 2, comprises two main\nmodules: (1) Token Mixing, which aggregates tokens using amplitude and\nphase information; and (2) Channel MLP Mixing, which extracts intra-token\nfeatures through two channel-wise fully connected layers with nonlinear ac-\ntivations. Token Mixing integrates a channel-wise fully connected layer\n(Channel-FC) with two Phase-Aware Token Mixing (PATM) modules op-\nerating along spatial dimensions. PATM plays the central role by modeling\neach token as a waveform and enabling interactions via both amplitude and\nphase. As shown in Eq. 2, these wave-like tokens are initially represented\n9\n\n--- Page 10 ---\nin the complex domain. To integrate them into an MLP-style architecture,\nwe can apply Euler\u2019s formula to unfold the complex expression into real and\nimaginary parts:\n\u02dczj=|zj| \u2299cos\u03b8j+i|zj| \u2299sin\u03b8j, j = 1,2,\u00b7\u00b7\u00b7, n. (3)\nThis formulation expresses each complex token as a pair of real-valued\nvectors, representing its real and imaginary components. These tokens \u02dczjare\nthen aggregated using the Token-FC operation:\n\u02dcoj=Token-FC (\u02dcF, Wt)j, j = 1,2,\u00b7\u00b7\u00b7, n, (4)\nwhere \u02dcF= [\u02dcf1,\u02dcf2,\u00b7\u00b7\u00b7,\u02dcfn]represents the set of all wave-like tokens. In this\nstep, tokens interact with one another by taking both amplitude and phase\ninto account. The result \u02dcojis the complex-valued output of the aggregated\ntoken. Following common quantum measurement approaches [44, 45], we can\nproject this complex output into the real domain by summing its real and\nimaginary parts with learnable weights:\noj=X\nkWt\njkfk\u2299cos\u03b8k+Wi\njkfk\u2299sin\u03b8k, j = 1,2,\u00b7\u00b7\u00b7, n, (5)\nwhere WtandWiare trainable weights. Here, the phase \u03b8kdynamically\nmodulates the aggregation process based on the semantic properties of the\ninput.\nTwo Phase-Aware Token Mixing (PATM) modules operate along the\nheight and width dimensions to capture spatial dependencies in both di-\nrections. These branches are combined through a reweighting mechanism\ninspired by CycleMLP, which adaptively fuses directional information. By\nrepresenting each token as a waveform and modulating it with learned phase\ninformation, our generator achieves both long-range spatial coherence and\nfine-grained stylistic control, enabling high-quality handwriting synthesis\nthat remains faithful to the input text and the writer\u2019s unique style.\n3.2.2. Dual Discriminator Architecture\nTo enhance the discriminative capability and address artifacts that may\nnot be readily apparent in the spatial domain alone, we employ a dual dis-\ncriminatorarchitectureconsistingoftwocomplementarycomponents: astan-\ndard spatial discriminator Dand a high-frequency discriminator DHF, as\nillustrated in Figure 3.\n10\n\n--- Page 11 ---\nFigure 3: Illustration of the Dual Discriminator Architecture.\nStandard Spatial Discriminator. The standard discriminator Dfollows the\nconventional adversarial framework, processing input handwriting images\nthrough a sequence of convolutional blocks. Each block consists of convo-\nlutional layers with spectral normalization, batch normalization, and ReLU\nactivations, progressively reducing spatial resolution while increasing feature\ndepth. This hierarchical feature extraction captures local textures, character\nshapes, and structural patterns at multiple scales. The final feature maps\nare globally averaged and passed through a linear layer to produce a spatial\nrealism score D(x), which reflects the plausibility of the image in the spatial\ndomain.\nHigh-Frequency Discriminator. To complement spatial analysis and target\nhigh-frequency artifacts common in generated handwriting, we introduce a\ndedicated high-frequency discriminator DHF. This discriminator operates\non frequency-decomposed representations of the input images rather than on\nraw pixel intensities. It employs a two-dimensional Haar wavelet transform\nimplemented via grouped convolutions with fixed Haar wavelet kernels. The\nwavelet decomposition for high-frequency components is formally expressed\nas:\nW(x) ={WLH, WHL, WHH}, (6)\nwhere the decomposition yields three sub-bands: WLH(low\u2013high), WHL\n(high\u2013low), and WHH(high\u2013high). Figure 4 shows that the high-frequency\nsub-bands ( WLH, WHL, WHH) capture detail coefficients corresponding to\nsharp edges, stroke boundaries, and fine contour variations that are cru-\ncial for authentic handwriting synthesis. The Haar wavelet basis functions\nare defined as:\nhL=1\u221a\n2[1,1], h H=1\u221a\n2[1,\u22121]. (7)\n11\n\n--- Page 12 ---\nFigure 4: Visualization of the transformed frequency components of handwriting images\nThe 2D wavelet filters are constructed as tensor products of the 1D basis\nfunctions: WLH=hT\nL\u2297hH,WHL=hT\nH\u2297hL, and WHH=hT\nH\u2297hH, where\n\u2297denotes the tensor product operation. This discriminator focuses on the\ndetail coefficients that capture fine-grained textures and structural patterns\ncharacteristic of authentic handwriting. The high-frequency representation\nis obtained by aggregating the detail sub-bands:\nH(x) =WLH+WHL+WHH. (8)\nThis aggregated high-frequency representation H(x)is then processed\nthrough the same convolutional architecture as the standard discriminator,\nyielding a frequency-domain realism score DHF(H(x))that evaluates the\nauthenticity of fine-scale features and textural details in the generated hand-\nwriting samples.\n3.2.3. Recognizer\nThe recognizer module Ris responsible for inferring the textual content y\nfrom handwriting images. It is trained solely on real, annotated handwriting\ndata and does not receive any supervision from synthetic examples. During\ngeneration, Racts as a semantic regulator by enforcing textual accuracy in\nthe output of the generator G, ensuring that the produced handwriting is not\nonly visually coherent but also linguistically faithful to the intended input\ntext.\n3.2.4. Style Encoder and Writer Identifier\nThe style encoder Eextracts stylistic attributes from handwritten word\nimages into a fixed-size latent vector sthat conditions the generator G. The\n12\n\n--- Page 13 ---\nwriter identifier Wpredicts author identity from handwriting images, pro-\nviding explicit stylistic supervision during training. Both modules share a\nfundamental objective: extracting style-relevant features while ignoring tex-\ntual content. This similarity motivates a shared backbone architecture that\nprocessesinputimagesthroughhierarchicalconvolutionallayerswithresidual\nconnections. The shared feature extraction reduces computational overhead\nwhile enabling both tasks to benefit from common visual representations.\nEach module employs a task-specific head after the shared backbone.\nThe style encoder uses a variational approach to model the style distribu-\ntion, while the writer identifier performs classification over writer identities.\nBothincorporatesequence-awareprocessingtohandlevariable-lengthinputs.\nThis design leverages multi-task learning benefits, where the complementary\nsupervision signals from style extraction and writer classification mutually\nimprove the quality of learned style representations.\n3.3. Objective Functions\nBefore training, our framework requires a dataset that provides multiple\nforms of supervision: visual samples of handwriting ( X), their associated\nlabels ( Y), and their corresponding author identifiers ( W). While these an-\nnotated samples serve as foundational data, our generative model must gen-\neralize beyond them, especially for synthesizing unseen or out-of-vocabulary\n(OOV) text. To address this, we draw textual inputs from a broader corpus,\nenabling the system to handle arbitrary word sequences during training by\nsampling a text sequence \u02dcyfrom the open corpus C. The complete training\nstrategy, including architecture and loss design, is outlined in Figure 1 and\ndetailed below.\n3.3.1. Adversarial Loss\nWe adopt the standard Generative Adversarial Network (GAN) frame-\nwork, where the discriminator Dlearns to differentiate between real hand-\nwriting samples and those synthesized by the generator G. This adversarial\nsetup encourages Gto produce more visually realistic handwriting. For sta-\nbilityandeffectiveness,weemploythewidelyusedhingelossformulation[46],\ndefined as:\nLadv=Ex\u223cX[max(0 ,1\u2212D(x))] +E\u02dcy\u223cC, z[max(0 ,1 +D(G(\u02dcy, z)))],(9)\nwhere the style feature zis obtained either by (1) sampling from a prior\nnormal distribution N(0,1), or (2) extracting it from a reference image xvia\n13\n\n--- Page 14 ---\nthe encoder, i.e., z=E(x). It is important to note that the adversarial loss\nfocusessolelyonimprovingthevisualrealismofthegeneratedimages. Itdoes\nnot enforce the preservation of textual content or the fidelity of calligraphic\nstyle, which are addressed by additional objectives in our framework.\n3.3.2. Text Recognition Loss\nTo ensure the synthesized handwriting conveys the intended textual con-\ntent, we introduce a handwriting recognizer Rto guide the generator G.\nWhile the adversarial loss promotes realism, it does not guarantee that the\ngenerated images preserve textual accuracy. The recognizer Raddresses this\ngap by explicitly aligning image content with textual targets. It is first\ntrained in a supervised manner using real handwriting samples x\u2208Xand\ntheir corresponding transcriptions y\u2208Y. We employ the Connectionist\nTemporal Classification (CTC) loss [47] to accommodate unsegmented se-\nquence learning, defined as:\nLD\nR=Ex,y[LCTC(R(x), y)], (10)\nwhere R(x)is the sequence of predicted character probabilities for image x,\nandyis the ground-truth label.\nOnce Ris trained, its parameters are frozen, and it serves as a perceptual\nguide to enforce textual fidelity in generated images. Specifically, for any\nsampled text \u02dcy\u2208 Cand style feature z, the generator Gis encouraged to\nproduce an image G(\u02dcy, z)such that R(G(\u02dcy, z))accurately predicts \u02dcy. This\nleads to the generator-side recognition loss:\nLG\nR=E\u02dcy,z[LCTC(R(G(\u02dcy, z)),\u02dcy)], (11)\nwhich is backpropagated through Gto improve its capacity to render syn-\ntactically accurate handwriting. This design ensures that generated samples\nnot only appear realistic but also encode the correct textual information.\n3.3.3. Writer Identification Loss\nWhile handwriting appearance is often visually diverse, it remains stylis-\ntically consistent for a given individual. This property enables us to leverage\nwriter identity as a proxy for style supervision. Since we lack explicit labels\nfor fine-grained style attributes (e.g., stroke thickness, slant, character cur-\nvature), we introduce a writer classifier Wto capture these underlying style\nrepresentations. The identifier Wis trained to predict the writer identity\n14\n\n--- Page 15 ---\nfrom real handwriting samples x\u2208Xwith associated writer labels w\u2208W.\nThis is achieved using a standard cross-entropy classification objective:\nLD\nW=Ex,w[\u2212logp(w|W(x))], (12)\nwhich encourages Wto learn writer-discriminative features that implicitly\nreflect style.\nOnce trained, Wis held fixed and used to supervise the generator G\nvia a feedback signal that enforces stylistic fidelity. Given a reference image\nx, we extract the style code z=E(x)and synthesize a new image G(\u02dcy, z)\nwith arbitrary content \u02dcy\u2208 C. We then require the generated sample to be\nclassified by Was having originated from the same writer as the reference:\nLG\nW=Ex,w,\u02dcy[\u2212logp(w|W(G(\u02dcy, E(x))))]. (13)\nThis feedback loop ensures that the generator adheres to the stylistic\nidentity encoded in the reference, even when synthesizing unseen textual\ninputs. It is important to note that the writer classifier Wis only trained on\nthe training set and may not generalize to authors outside this domain (e.g.,\nunseen writers in the test set).\n3.3.4. Style Reconstruction Loss\nTo ensure that the generator genuinely incorporates the intended style\nfeature during synthesis, we introduce a reconstruction constraint that en-\nforces consistency between the input style and the one inferred back from the\ngenerated image, similar to the method proposed in [48]. This is essential\nfor learning an invertible mapping between the latent style space and visual\nhandwriting appearance. Given a style vector z\u223c N(0,1)and an arbitrary\ntext\u02dcy\u2208 C, the generator Gsynthesizes an image G(\u02dcy, z). We then re-encode\nthis image using the style encoder Eand require that the recovered style\nclosely match the original:\nLstyle=E\u02dcy,z[\u2225z\u2212E(G(\u02dcy, z))\u22251]. (14)\nThis self-consistency loss encourages the model to meaningfully utilize\nthe style vector zduring generation. It also promotes diversity in the output\nspace and helps prevent the generator from collapsing into a limited set of\nstyles.\n15\n\n--- Page 16 ---\n3.3.5. KL-Divergence Loss\nToenablestochasticstylesamplingduringinference,weenforcethelearned\nstyle space to follow a predefined prior distribution. This is achieved by ap-\nplying a Kullback-Leibler (KL) divergence penalty between the distribution\nof encoded style vectors and a standard normal prior. Given a real hand-\nwriting image x\u2208X, we compute the KL divergence between the posterior\ndistribution E(x)and the prior N(0,1):\nLkl=Ex[DKL(E(x)\u2225N(0,1))]. (15)\nThis loss acts as a regularizer that encourages the latent style space to\nremaincontinuous, smooth, andsampleableattesttime. Itiscommonlyused\nin variational frameworks and has proven effective in style transfer tasks [49,\n50].\n3.3.6. Frequency Distribution Loss\nStandard image synthesis losses, such as pixel-wise \u21131or perceptual losses\nbased on spatial features (e.g., Gram or contextual loss), typically assume\nspatial alignment between predicted and reference images. However, this\nassumption often breaks down in handwriting synthesis, where stylistic at-\ntributes, such as character shape, stroke curvature, and slant, can vary in-\ndependently of exact spatial positioning. Additionally, content mismatches\nand varying text lengths further contribute to spatial misalignments between\ngenerated and real samples.\nToaddresstheseissues,weadopttheFrequencyDistributionLoss(FDL)[9],\napplied on high-level feature representations . The key idea is to treat an im-\nage as a set of semantic features and to measure similarity between distribu-\ntions of these features while ignoring their spatial locations. This encourages\nstylistic consistency while allowing minor spatial variations. Specifically, we\nextract deep features from a fixed network \u03a6(e.g., the l-th layer of the\nshared backbone of writer identifier and style encoder), treating the outputs\nas unordered sets of feature vectors:\nA={a1, a2, . . . , a N}= \u03a6l(x), B ={b1, b2, . . . , b N}= \u03a6l(G(y, E(x))),\n(16)\nwhere xis a real handwriting image, and G(y, E(x))is the reconstruction\ngenerated from the ground-truth content yand the encoded style E(x).\nWe apply the Discrete Fourier Transform (DFT) to the spatial axes of\neach feature map in AandB, decomposing them into frequency domain rep-\n16\n\n--- Page 17 ---\nresentations. Specifically, each feature vector is transformed into a complex-\nvalued representation from which we derive amplitude and phase compo-\nnents. Let A(A)andP(A)denote the sets of amplitudes and phases com-\nputed from A, and similarly A(B)andP(B)from B. To compare these\ndistributions while accounting for local style variations and global structure,\nwe apply the Sliced Wasserstein Distance (SWD) separately on amplitudes\nand phases:\nLFDL= SW( A(A),A(B)) +\u03bb\u00b7SW(P(A),P(B)), (17)\nwhere SW(\u00b7,\u00b7)denotes the SWD between two empirical distributions. The\nhyperparameter \u03bbbalances the contribution of amplitude and phase statis-\ntics.\n3.3.7. High-Frequency Adversarial Loss\nWhile the standard adversarial loss encourages global visual realism,\nhandwriting synthesis also depends on fine-grained attributes such as stroke\ntexture, pen pressure variations, and sharp character edges. To explicitly en-\nforce realism in these high-frequency details, we leverage the high-frequency\ndiscriminator DHFdescribed in 3.2.2, which operates on the three high-\nfrequency sub-bands (LH, HL, HH) obtained from a 2D Haar wavelet de-\ncomposition. Given an input image x, the high-frequency representation\nis computed as xHF=WLH(x) +WHL(x) +WHH(x)where WLH,WHL,\nandWHHcorrespond to detail coefficients that encode sharp edges, stroke\nboundaries, and fine contour variations. The high-frequency adversarial loss\nadopts the same hinge loss formulation as the standard adversarial loss, but\nis applied to xHF:\nLHF=Ex\u223cX[max(0 ,1\u2212DHF(xHF))]+E\u02dcy\u223cC, z[max(0 ,1 +DHF(GHF(\u02dcy, z)))],\n(18)\nwhere GHF(\u02dcy, z)denotes the high-frequency components of the generated\nimage G(\u02dcy, z)obtained using the same decomposition.\nFor generator training, LHFencourages the generator to produce im-\nages with realistic textures and clearly defined character edges. In our dual-\ndiscriminator framework, Ladvprovides global structural supervision, while\nLHFcomplements it with targeted supervision on fine-grained details, en-\nsuring handwriting outputs are both structurally coherent and texturally\nauthentic.\n17\n\n--- Page 18 ---\n3.3.8. Overall Objectives\nOur model is trained via a min-max adversarial game that balances gen-\nerator and discriminator objectives, along with auxiliary supervision tasks.\nThe complete training process can be summarized as follows.\nDiscriminator and Auxiliary Modules.. Whenmaximizingtheadversarialob-\njective, we update the global discriminator D, Recognizer R, and Writer\nIdentifier Windependently. Their optimization goals are defined as:\nLD=\u2212Ladv,LDHF=\u2212LHF,LR=LD\nR,LW=LD\nW.(19)\nGenerator and Style Encoder.. During the minimization phase, the generator\nGand the style encoder Eare jointly optimized to minimize the following\ncomposite loss:\nLG,E=Ladv+LHF+\u03bbRLG\nR+\u03bbWLG\nW+\u03bbstyleLstyle+\u03bbklLkl+\u03bbFDLLFDL.\n(20)\nEach \u03bbis a weighting coefficient that balances the influence of its corre-\nsponding loss term. These terms supervise different aspects of handwriting\nsynthesis, including realism, legibility, style fidelity, and latent space regu-\nlarization.\n4. Experiments\nWe evaluate the proposed FW-GAN framework through a comprehen-\nsive set of experiments designed to assess both generation quality and prac-\ntical utility. This section first outlines the design of our evaluation procedure\nand implementation details, followed by comparisons against state-of-the-art\nbaselines, ablation studies, and cross-language generalization tests. We also\ninvestigate the model\u2019s impact on downstream handwriting recognition tasks\nand examine its deployment efficiency.\n4.1. Design of Evaluation Procedure\nProgress in handwritten text generation (HTG) has been slowed by the\nlack of a unified benchmark for performance assessment. The diversity in\nevaluation setups across different studies makes it difficult to establish con-\nsistent comparisons or draw general conclusions. To tackle this, we propose\na clear and replicable evaluation scheme tailored to the specific challenges of\nHTG.\n18\n\n--- Page 19 ---\nWhile the proposed evaluation framework is general-purpose and com-\npatible with multiple datasets, we ground our description using the IAM\ndataset due to its frequent use in the literature. This dataset contains about\n62,857 handwritten English word images produced by 500 individual writers.\nRather than relying on the original split, where writers are partitioned into\ntraining, validation, and test groups, we adopt the revised division widely\nused in works such as HWT [6] and VATr [27], which assigns 339 writers to\ntraining and the remaining 161 to testing.\nFor consistency with prior models, we use the same preprocessed version\nof the dataset provided by HWT and VATr. In this version, all images are\nstandardized to a height of 32 pixels, with 16 pixels allocated per character in\nthe text label. To handle varying word lengths, images with a width smaller\nthan 128 pixels are padded, while those exceeding 128 pixels are resized\nto a fixed size of 32\u00d7128pixels. To comprehensively evaluate generative\nperformance, we outline five controlled testing scenarios that challenge both\nlexical and stylistic generalization:\n\u2022Test Set Replication: In this setting, the evaluation procedure re-\nconstructs the entire test set by generating each target word exactly\nas it appears in the original test set. For each generated sample, the\nstyle is derived from reference images of the same writer containing the\nsame word, enabling an assessment of the model\u2019s ability to faithfully\nreproduce both content and writer-specific stylistic attributes.\n\u2022IV-S (In-Vocabulary, Seen Style): Generate words that appear in\nthe training set using handwriting styles from writers the model has\nencountered during training.\n\u2022IV-U (In-Vocabulary, Unseen Style): Synthesize known words\nusing styles from previously unseen writers.\n\u2022OOV-S (Out-of-Vocabulary, Seen Style): Produce words outside\nthe training vocabulary, rendered in familiar writing styles.\n\u2022OOV-U (Out-of-Vocabulary, Unseen Style): Combine novel vo-\ncabulary with entirely new handwriting styles, posing a dual general-\nization challenge.\nIn constructing the last four evaluation corpora, we prepare two distinct\nword pools: one containing in-vocabulary words selected from the training\n19\n\n--- Page 20 ---\ndata, and another containing out-of-vocabulary words drawn from an exter-\nnal English corpus. For each configuration, the model is used to generate\nabout 25,000 synthetic word images. These outputs are then compared to\nreal samples using quantitative metrics.\nTo objectively assess the quality of the generated content, we employ\nboth perceptual and stylistic similarity measures. Specifically, we compute\nthe Fr\u00e9chet Inception Distance (FID) and Kernel Inception Distance (KID)\nto capture visual fidelity. These metrics are calculated between real and\ngenerated image distributions across the different settings.\n4.2. Implementation Details\nOur model is implemented using the PyTorch framework1and all ex-\nperiments are conducted on a single NVIDIA Tesla P100 GPU. Training is\nperformed using the Adam optimizer [51], with a learning rate initialized at\n0.0002and momentum parameters set to (\u03b21, \u03b22) = (0 .5,0.999). Starting\nfrom the 35thepoch, the learning rate is gradually reduced following a linear\ndecay schedule.\nFor the FW-GAN model, we apply a fixed weight of \u03bbKL= 0.0001for the\nKL divergence term and set \u03bbFDL= 1for the frequency distribution loss. The\nremaining loss coefficients are adaptively tuned throughout training using\na gradient balancing strategy, allowing the model to dynamically regulate\ncompeting objectives.\n4.3. Benchmarking Against State-of-the-Art Methods\nTo assess the effectiveness of our proposed model, we conduct a compara-\ntiveanalysiswithseveralleadingapproachesinstyle-conditionedhandwritten\ntext generation. Specifically, we include the Transformer-based Handwriting\nTransformer (HWT) [6], VATr [27], and the more recent HiGAN [21] and\nHiGAN+ [43] in our evaluation.\nTo maintain a fair and consistent experimental setup, we adopt the fol-\nlowing protocol: for methods such as HWT and VATr that offer publicly\navailable pretrained models, we utilize their released checkpoints trained on\nthe same dataset splits. For other baselines like HiGAN and HiGAN+, we\nre-implement and train them from scratch using the identical data partitions\nemployed in HWT and VATr. This ensures that all models are evaluated\n1https://pytorch.org/\n20\n\n--- Page 21 ---\nunder the same conditions, eliminating any discrepancies due to differing\npreprocessing or training schemes. We report quantitative comparisons us-\ning two key metrics: Fr\u00e9chet Inception Distance (FID) and Kernel Inception\nDistance (KID), as shown in table 1 and 2. Across all test scenarios, our\nmodel (FW-GAN) consistently achieves superior results in terms of FID and\nKID, indicating both higher visual fidelity and stylistic coherence compared\nto existing methods.\nTable 1 shows that our model achieves the lowest scores on both evalu-\nation metrics, with a FID of 6.530 and KID of 0.20, indicating the highest\nsimilarity to real handwriting among all compared methods. In contrast,\nHiGAN and HiGAN+ report considerably higher FID scores of 17.086 and\n16.114, respectively, and KID scores of 1.18 and 0.81, showing a clear per-\nformance gap. Transformer-based models, such as HWT (FID: 13.615, KID:\n0.49) and VATr (FID: 13.577, KID: 0.47), perform better than earlier meth-\nods but still fall short of our results. Notably, our FID is less than half that\nof the next best-performing model (VATr), and our KID is similarly reduced\nby more than 50%.\nTable 2 reports FID scores across four evaluation scenarios designed to as-\nsess both lexical generalization and stylistic robustness. Our model achieves\nthe lowest FID in all four conditions: 24.92 (IV-S), 28.04 (IV-U), 25.46\n(OOV-S), and 28.80 (OOV-U). In the IV-S setting, our method improves\nover the next best model, HWT (26.46), by 1.54 points, and outperforms Hi-\nGAN (36.02) by more than 11 points. In IV-U, we surpass VATr (29.94) by\n1.90 points, while achieving over 9-point gains compared to HiGAN variants,\ndemonstrating strong generalization of style to unseen writers while retaining\nlexical familiarity. In the OOV-S scenario, where new words are synthesized\nin seen styles, our approach leads by 1.01 points over HWT and 1.36 points\nover VATr. Most notably, in the most challenging OOV-U setting\u2014where\nboth lexical content and style are unseen\u2014our method records 28.80, out-\nperforming VATr (29.50) by 0.70 points, HWT (29.68) by 0.88 points, and\nHiGAN+ (37.75) by nearly 9 points. While HWT and VATr remain com-\npetitive in some OOV cases, neither achieves consistent superiority across\nall settings. In contrast, our method maintains stable and low FID scores\nunder all conditions, underscoring its robustness to domain shifts and its ef-\nfectiveness in producing high-fidelity handwriting from familiar contexts to\nthe most challenging OOV-U scenario.\nThese findings underscore the strength of our proposed model in handling\nreal-world handwriting generation tasks where both the text content and the\n21\n\n--- Page 22 ---\nTable 1: Quantitative comparison of handwriting synthesis quality using FID and KID\nmetrics. Lower values indicate higher similarity to real handwriting.\nMethod FID KID\nHiGAN [21] 17.086 1.18\nHiGAN+ [43] 16.114 0.81\nHWT [6] 13.615 0.49\nVATr [27] 13.577 0.47\nOurs 6.530 0.20\nTable 2: FID scores under different evaluation conditions: In/Out-of-Vocabulary and\nSeen/Unseen styles (lower is better). Each condition includes 25,000 generated samples.\nMethod IV-S IV-U OOV-S OOV-U\nHiGAN [21] 36.02 40.94 36.54 41.35\nHiGAN+ [43] 34.90 37.78 34.66 37.75\nHWT [6] 26.46 29.86 26.47 29.68\nVATr [27] 26.73 29.94 26.82 29.50\nOurs 24.92 28.04 25.46 28.80\nwriter\u2019s style may vary significantly. The ability to generalize under such\nconditions is essential for practical deployments, and our results reflect a\nrobust and adaptive solution for style-driven handwritten text generation.\n4.4. Enhancing HTR Performance via Synthetic Data\nTo further validate the practical utility of our handwriting synthesis\nmodel, we examine its impact on downstream Handwritten Text Recogni-\ntion (HTR) performance, particularly in scenarios where real training data\nisscarce. Synthetichandwritinggenerationaimsnotonlytoproducerealistic\nimages but also to support recognition tasks by serving as effective training\ndata.\nTo simulate a low-resource setting, we begin by training a baseline HTR\nmodel using only 5,000 real handwriting samples from the IAM dataset,\nwhich the model has never seen before. For this experiment, we adopt a\nTransformer-based OCR architecture following the TrOCR framework [52].\nWe then expand the training dataset by incorporating 25,000 synthetic sam-\nples generated by each competing model, including ours, generated from the\n22\n\n--- Page 23 ---\n5,000 real images with a random lexicon. This augmented dataset (5,000 real\n+ 25,000 synthetic images) is used to retrain the HTR model. By keeping\nthe real data fixed and varying only the synthetic augmentation source, we\ncan directly evaluate the contribution of each synthesis method to recogni-\ntion performance. We assess HTR effectiveness using three widely accepted\nmetrics:\n\u2022Character Error Rate (CER): Measurestheproportionofcharacter-\nlevel errors\u2014substitutions, insertions, and deletions\u2014relative to the\ntotal number of characters in the ground truth.\n\u2022Word Error Rate (WER): Computes the proportion of incorrectly\npredicted words, capturing recognition quality at the word level.\n\u2022Normalized Edit Distance (NED): Quantifies the normalized dif-\nference between predicted and target sequences; lower values indicate\nhigher transcription fidelity.\nAs shown in Table 3, all synthetic augmentation methods yield substan-\ntial performance gains over the baseline trained with only 5,000 real images,\ndemonstrating the effectiveness of synthetic handwriting in compensating for\ndata scarcity. Among competing approaches, our method achieves the low-\nest error rates across all three metrics, with a CER of 10.23, NED of 10.09,\nand WER of 28.18. Compared to the best-performing baseline competi-\ntor (VATr), our method reduces CER by 0.39 points, NED by 0.32 points,\nand WER by 1.47 points. These improvements are consistent across both\ncharacter- and word-level metrics, indicating that our generated samples not\nonly enhance fine-grained recognition accuracy but also improve overall lexi-\ncal prediction. The relatively large WER reduction suggests that our model\nproduces more semantically coherent handwriting, enabling the HTR model\nto better capture entire word structures rather than just isolated characters.\nThis performance consistency across multiple metrics reinforces that our syn-\nthesisapproachgeneratesdatathatisbothvisuallyrealisticandlinguistically\neffective for training robust recognition models.\n4.5. Ablation Study\nTo evaluate the contribution of each component in our proposed FW-\nGANframework, we conduct an ablation study on the IAM dataset. We\nstart with a baseline configuration (A), which uses only a BigGAN generator\n23\n\n--- Page 24 ---\nTable 3: HTR performance on the IAM dataset using 5,000 real images and 25,000 syn-\nthetic images from each method. Lower scores indicate better performance.\nMethod CER NED WER\n5000 real images 12.25 11.95 32.81\nHiGAN 10.88 10.48 29.28\nHiGAN+ 10.67 10.51 29.65\nHWT 10.65 10.51 29.71\nVATr 10.62 10.41 29.65\nOurs 10.23 10.09 28.18\nbackbone, and incrementally add components: Frequency Distribution Loss\n(FDL), Wave-Modulated Generator, and High-Frequency Discriminator. We\nassess their impact on generation quality using Fr\u00e9chet Inception Distance\n(FID) and downstream recognition performance using Character Error Rate\n(CER), Normalized Edit Distance (NED), and Word Error Rate (WER).\nTable 4 summarizes the results. The baseline (A) achieves an FID of\n15.94, with CER of 10.57, NED of 10.40, and WER of 29.34. Adding FDL in\nconfiguration (B) reduces FID to 10.20, indicating enhanced visual realism,\nbut slightly increases CER (10.74) and NED (10.50), suggesting a minor\ntrade-off in character-level fidelity. Replacing the generator with our Wave-\nModulated architecture in configuration (C) further lowers FID to 6.89 and\nimproves recognition performance (CER: 10.32, NED: 10.12, WER: 28.81),\ndemonstrating benefits to both visual quality and text accuracy. Finally,\nincorporating the High-Frequency Discriminator in configuration (D) yields\nthe best results, with an FID of 6.53, CER of 10.23, NED of 10.09, and WER\nof 28.18, highlighting its role in capturing fine-grained details that enhance\nboth realism and recognizability.\nTo further analyze these improvements, we visualize the generated hand-\nwriting images in Figure 5. In configuration (A), using only the BigGAN\ngenerator backbone, the generated text often appears blurry or faint, partic-\nularly in stroke edges (highlighted in red boxes), due to the model\u2019s limited\nability to capture fine-grained details. Introducing the Frequency Distribu-\ntion Loss (FDL) in configuration (B) reduces blurriness and enhances text\nclarity by aligning the spectral characteristics of generated images with real\nsamples, leveraging high-frequency feature distributions to guide the gen-\nerator toward sharper stroke details. However, stroke color and style still\n24\n\n--- Page 25 ---\nTable 4: Ablation study evaluating the effect of each architectural component on gener-\nation and recognition performance (IAM dataset). Lower values indicate better perfor-\nmance.\nModel FID CER NED WER\nBase (BigGAN backbone) (A) 15.94 10.57 10.40 29.34\n(A) + Frequency Distribution Loss (B) 10.20 10.74 10.50 29.05\n(B) + Wave-Modulated Generator (C) 6.89 10.32 10.12 28.81\n(C) + High-Frequency Discriminator (D) 6.53 10.23 10.09 28.18\ndeviate from the reference, indicating incomplete style capture. Incorpo-\nrating the Wave-Modulated Generator in configuration (C) significantly im-\nproves stroke fidelity and color consistency (highlighted in green boxes) by\nusing phase-aware Wave-MLP modules to capture stylistic patterns, though\nthe tail of the character \u2019y\u2019 occasionally remains faint. Finally, adding the\nHigh-FrequencyDiscriminatorinconfiguration(D)producestextwithsharp,\nclear strokes and colors closely matching the reference style, fully eliminat-\ning blurriness, including in the character \u2018y\u2019. This is achieved by leveraging\nwavelet-decomposed high-frequency components to detect and correct subtle\nartifacts, ensuring precise stroke rendering and stylistic fidelity.\n4.6. Cross-Language Generalization: Vietnamese Handwriting\nTo evaluate the generalization ability and robustness of FW-GAN across\ndifferent writing systems, we extend our experiments beyond English and ap-\nplythesameexperimentalsetupdescribedin4.2totheVietnameselanguage.\nWe use the HANDS-VNOnDB dataset [1], which contains 7,296 handwritten\ntext lines comprising over 480,000 strokes and more than 380,000 characters.\nFollowing the dataset\u2019s standard split, we train on handwriting from 106\nwriters and test on 34 unseen writers.\nAs reported in Table 5, our method achieves the lowest FID (5.61) and\nKID(0.30)amongallevaluatedapproaches. Comparedtothebest-performing\nbaseline, HWT, FW-GAN reduces FID by 4.24 points and lowers KID from\n0.72 to 0.30. These improvements are consistent across both metrics, indi-\ncating that our model produces handwriting that is visually closer to real\nsamples while also matching the underlying feature distribution more ac-\ncurately. Notably, Vietnamese handwriting introduces challenges absent in\nEnglish, including extensive use of diacritical marks, compound vowels, and\n25\n\n--- Page 26 ---\nFigure 5: Visualization of handwriting images generated by each architectural component,\nusing a single reference image (\u2019work\u2019) to generate a sentence. Red boxes highlight failure\ncases with blurry or faint text; green boxes indicate improved text clarity, stroke style,\nand color fidelity compared to prior configurations. Models A, B, C, and D correspond to\nconfigurations in Table 4.\nTable 5: Quantitative comparison of handwriting synthesis quality using FID and KID\nmetrics on HANDS-VNOnDB. Lower values indicate higher similarity to real handwriting.\nMethod FID KID\nVATr [27] 23.88 2.72\nHiGAN [21] 11.26 0.79\nHWT [6] 9.85 0.72\nOurs 5.61 0.30\ntonal variations, all of which increase spatial complexity and stylistic di-\nversity. Despite these complexities, FW-GAN successfully captures fine-\ngrained structural and stylistic cues, producing images that are both visually\nconvincing and stylistically coherent across writers. This strong performance\non a typologically different language demonstrates the model\u2019s robustness\nin cross-language scenarios, confirming its ability to generalize high-fidelity\nsynthesis beyond the training language.\n26\n\n--- Page 27 ---\nTable 6: Comparison of model sizes (in megabytes) for handwriting synthesis. Only the\ngenerator (Gen) and encoder (Enc) modules are considered.\nMethod Gen (MB) Enc (MB) Total (MB)\nHWT [6] 80.7 50.6 131.3\nVATr [27] 113.11 42.61 155.72\nHiGAN [21] 38.6 20.5 59.1\nHiGAN+ [43] 15.0 6.7 21.7\nOurs 46.37 6.59 52.96\n4.7. Model Size and Deployment Efficiency\nWe assess the parameter efficiency of our proposed model by comparing\nits memory footprint against representative handwriting synthesis baselines,\nas detailed in 6. Our model exhibits a total size of just 52.96 MB, placing it\namong the most compact solutions\u2014substantially smaller than HWT (131.3\nMB), VATr (155.72 MB), and only marginally larger than HiGAN+ (21.7\nMB). The compact nature of our architecture makes it highly suitable for\nreal-world use cases where computational resources and memory are limited.\nDespite its smaller size, our model maintains high-quality output and strong\nrecognition compatibility, offering a favorable trade-off between efficiency\nand performance.\n4.8. Visual Comparison\nIn addition to the quantitative evaluations presented in previous sections,\nweconductaqualitativeanalysistofurtherassessthevisualrealismandstyle\npreservation capability of FW-GAN . While metrics such as FID, KID, and\nrecognition-based scores provide objective measures of performance, they do\nnot fully capture subtle handwriting characteristics such as stroke continu-\nity, letter slant, or spacing regularity. To address this, we visually compare\nour model with several state-of-the-art baselines under two complementary\nsettings: generation quality, where models synthesize handwriting given a\nreference style and new content, and reconstruction fidelity, where models\naim to replicate a target style as closely as possible. The following sub-\nsections present side-by-side examples highlighting key visual differences be-\ntween methods.\n27\n\n--- Page 28 ---\nFigure 6: Qualitative comparison of handwriting generated from the IAM dataset across\nmodels, using identical style and content inputs. Each row corresponds to a different\nmodel, with red boxes highlighting failure cases (e.g., blurry or incorrect characters). The\nasterisk (*) denotes a one-shot model requiring only a single reference image.\nFigure 7: Qualitative comparison of handwriting generated from the HANDS-VNOnDB\ndataset across models, using identical style and content inputs. Each row corresponds\nto a different model, with red boxes highlighting failure cases (e.g., blurry or incorrect\ncharacters). The asterisk (*) denotes a one-shot model requiring only a single reference\nimage.\n4.8.1. Generation Quality\nFigures 6 and 7 present qualitative comparisons of handwriting gener-\nated by our FW-GAN model against baselines, including HiGAN [21], Hi-\nGAN+ [43], HWT [6], and VATr [27], on the English (IAM) and Vietnamese\n(HANDS-VNOnDB) datasets. All models were conditioned on identical style\nreferences (top row) and content inputs (bottom row). Our method consis-\n28\n\n--- Page 29 ---\nFigure 8: Qualitative reconstruction results. Each row corresponds to a different model,\nconditioned on the same ground-truth style (top row) and target text (bottom row). The\ngoal is to reproduce the target content while preserving the handwriting style.\ntently produces stylistically coherent and visually clear handwriting across\nboth datasets.\nFor the IAM dataset, FW-GAN generates text with smooth stroke con-\ntinuity, accurate stroke thickness, and color fidelity, closely matching the\nreference style. Competing models, such as HiGAN and VATr, exhibit char-\nacter errors (e.g., blurry or distorted \u2019s\u2019 and \u2019z\u2019, highlighted in red boxes) and\ninconsistent color or shading. In contrast, our model\u2019s phase-aware Wave-\nMLP and frequency-guided components ensure legible characters with pre-\ncise stylistic attributes. For the HANDS-VNOnDB dataset, where complex\ncharacter shapes and diacritics pose additional challenges, FW-GAN signifi-\ncantly outperforms baselines. While VATr and HWT produce broken strokes\nand diacritic misplacements (highlighted in red boxes), our model leverages\nits Frequency Distribution Loss and High-Frequency Discriminator to gen-\nerate sharp, coherent text with accurate slant, hue, and stroke thickness,\ndemonstrating robust generalization across languages.\n4.8.2. Reconstruction Fidelity\nFigure8presentsreconstructionresultswhereeachmodelisgivenground-\ntruth style images along with a target content prompt, with the goal of\nreproducing handwriting that closely matches the reference style.\nOur model preserves character size, color, and brightness with high fi-\n29\n\n--- Page 30 ---\ndelity, while capturing the handwriting slant more accurately than others.\nIt also avoids common issues such as stroke omission, for instance, in Hi-\nGAN\u2019s rendering of the letter \u201ct\u201d in \u201cwhite,\u201d where the crossbar is missing.\nIn comparison, HWT often produces overly bold characters (e.g., in \u201cFor-\nrest\u201d or \u201cattention\u201d), whereas VATr tends to generate strokes that appear\nless cohesive and somewhat fragmented.\nBy contrast, our method consistently produces smooth, well-connected\nletters that align with the reference style in both color tone and stroke inten-\nsity. While HiGAN and HiGAN+ deliver relatively strong performance, our\napproach more faithfully reproduces subtle stylistic nuances, such as slight\nvariations in stroke darkness and character shapes. These results demon-\nstrate the effectiveness of our Frequency-Driven and Wave-Modulated MLP\nGenerator in capturing fine-grained handwriting details.\n5. Conclusion\nIn this paper, we introduce FW-GAN , a novel GAN-based model de-\nsigned for one-shot handwriting synthesis, addressing the challenge of gen-\nerating realistic, style-consistent handwritten text from a single sample. By\nintegrating phase-aware Wave-MLP modules into the generator, our model\ncaptures intricate calligraphic styles, overcoming the limitations of conven-\ntional convolutional architectures. A frequency-guided discriminator, paired\nwith a novel Frequency Distribution Loss, aligns the spectral characteristics\nof generated samples with authentic handwriting, enhancing fine-grained de-\ntails such as stroke edges and pen pressure variations. To the best of our\nknowledge, this work represents one of the first explorations of frequency-\naware mechanisms and MLP-augmented architectures in handwriting syn-\nthesis. The proposed framework not only advances the field of generative\nmodeling for handwriting but also provides a robust tool for data augmen-\ntation in HTR pipelines and personalized text generation.\nReferences\n[1] H. T. Nguyen, C. T. Nguyen, P. T. Bao, M. Nakagawa, A database of\nunconstrained vietnamese online handwriting and recognition exper-\niments by recurrent neural networks, Pattern Recognition 78 (2018)\n291\u2013306. doi:https://doi.org/10.1016/j.patcog.2018.01.013 .\nURL https://www.sciencedirect.com/science/article/pii/\nS0031320318300141\n30\n\n--- Page 31 ---\n[2] F. Kleber, S. Fiel, M. Diem, R. Sablatnig, CVL-DataBase: An Off-Line\nDatabase for Writer Retrieval, Writer Identification and Word Spotting,\nin: ICDAR, 2013.\n[3] I. Pratikakis, K. Zagori, P. Kaddas, B. Gatos, Icfhr 2018 competition\non handwritten document image binarization (h-dibco 2018), in: 2018\n16th International Conference on Frontiers in Handwriting Recognition\n(ICFHR), 2018, pp. 489\u2013493. doi:10.1109/ICFHR-2018.2018.00091 .\n[4] R. D. Lins, Nabuco - two decades of document processing in latin amer-\nica, J. Univers. Comput. Sci. 17 (2011) 151\u2013161.\nURL https://api.semanticscholar.org/CorpusID:2896293\n[5] A. Graves, Generating Sequences with Recurrent Neural Networks,\narXiv preprint arXiv:1308.0850 (2013).\n[6] A. K. Bhunia, S. Khan, H. Cholakkal, R. M. Anwer, F. S. Khan,\nM. Shah, Handwriting Transformers, in: ICCV, 2021.\n[7] Z.-Q. J. X. Zhi-Qin John Xu, Y. Z. Yaoyu Zhang, T. L. Tao Luo, Y. X.\nYanyang Xiao, Z. M. Zheng Ma, Frequency principle: Fourier analysis\nsheds light on deep neural networks, Communications in Computational\nPhysics 28 (5) (2020) 1746\u20131767. doi:10.4208/cicp.oa-2020-0085 .\nURL http://dx.doi.org/10.4208/cicp.OA-2020-0085\n[8] Y. Tang, K. Han, J. Guo, C. Xu, Y. Li, C. Xu, Y. Wang, An image patch\nis a wave: Phase-aware vision mlp, in: 2022 IEEE/CVF Conference\non Computer Vision and Pattern Recognition (CVPR), IEEE, 2022, p.\n10925\u201310934. doi:10.1109/cvpr52688.2022.01066 .\nURL http://dx.doi.org/10.1109/CVPR52688.2022.01066\n[9] Z. Ni, J. Wu, Z. Wang, W. Yang, H. Wang, L. Ma, Misalignment-\nrobust frequency distribution loss for image transformation, in: 2024\nIEEE/CVF Conference on Computer Vision and Pattern Recognition\n(CVPR), IEEE, 2024, p. 2910\u20132919. doi:10.1109/cvpr52733.2024.\n00281.\nURL http://dx.doi.org/10.1109/CVPR52733.2024.00281\n[10] E. Aksan, F. Pece, O. Hilliges, DeepWriting: Making digital ink editable\nvia deep generative modeling, in: CHI, ACM, 2018.\n31\n\n--- Page 32 ---\n[11] E. Aksan, O. Hilliges, STCN: Stochastic Temporal Convolutional Net-\nworks, in: ICLR, 2018.\n[12] A. Kotani, S. Tellex, J. Tompkin, Generating Handwriting via Decou-\npled Style Descriptors, in: ECCV, 2020.\n[13] B. Ji, T. Chen, Generative Adversarial Network for Handwritten Text,\narXiv preprint arXiv:1907.11845 (2019).\n[14] J. Wang, C. Wu, Y.-Q. Xu, H.-Y. Shum, Combining Shape and Physical\nModels for On-line Cursive Handwriting Synthesis, IJDAR 7 (4) (2005)\n219\u2013227.\n[15] Z. Lin, L. Wan, Style-preserving english handwriting synthesis, Pattern\nRecognit. 40 (7) (2007) 2097\u20132109.\n[16] A. O. Thomas, A. Rusu, V. Govindaraju, Synthetic Handwritten\nCAPTCHAs, Pattern Recognit. 42 (12) (2009) 3365\u20133373.\n[17] T. Haines, O. Mac Aodha, G. Brostow, My Text in Your Handwriting,\nACM Trans. Graphics 35 (3) (2016).\n[18] E.Alonso, B.Moysset, R.Messina, AdversarialGenerationofHandwrit-\ntenTextImagesConditionedonSequences,in: ICDAR,IEEEComputer\nSociety, 2019.\n[19] S. Fogel, H. Averbuch-Elor, S. Cohen, S. Mazor, R. Litman, Scrabble-\nGAN: Semi-Supervised Varying Length Handwritten Text Generation,\nin: CVPR, 2020.\n[20] L. Kang, P. Riba, Y. Wang, M. Rusi\u02dc nol, A. Forn\u00e9s, M. Villegas, GAN-\nwriting: Content-Conditioned Generation of Styled Handwritten Word\nImages, in: ECCV, 2020.\n[21] J. Gan, W. Wang, HiGAN: Handwriting Imitation Conditioned on\nArbitrary-Length Texts and Disentangled Styles, in: AAAI, 2021.\n[22] A. Mattick, M. Mayr, M. Seuret, A. Maier, V. Christlein, SmartPatch:\nImproving Handwritten Word Imitation with Patch Discriminators, in:\nICDAR, 2021.\n32\n\n--- Page 33 ---\n[23] C. Luo, Y. Zhu, L. Jin, Z. Li, D. Peng, SLOGAN: Handwriting\nStyleSynthesisforArbitrary-LengthandOut-of-VocabularyText, IEEE\nTrans. Neural Netw. Learn. Syst. (2022).\n[24] P. Krishnan, R. Kovvuri, G. Pang, B. Vassilev, T. Hassner, TextStyle-\nBrush: Transfer of Text Aesthetics from a Single Example, arXiv e-\nprints (2021) arXiv\u20132106.\n[25] B. Davis, C. Tensmeyer, B. Price, C. Wigington, B. Morse, R. Jain,\nText and Style Conditioned GAN for Generation of Offline Handwriting\nLines, in: BMVC, 2020.\n[26] A. Brock, J. Donahue, K. Simonyan, Large Scale GAN Training for High\nFidelity Natural Image Synthesis, in: ICLR, 2019.\n[27] V. Pippi, S. Cascianelli, R. Cucchiara, Handwritten Text Generation\nfrom Visual Archetypes, in: CVPR, 2023.\n[28] I. Tolstikhin, N. Houlsby, A. Kolesnikov, L. Beyer, X. Zhai, T. Un-\nterthiner, J. Yung, A. Steiner, D. Keysers, J. Uszkoreit, M. Lucic,\nA. Dosovitskiy, Mlp-mixer: An all-mlp architecture for vision (2021).\narXiv:2105.01601 .\n[29] X. Ding, C. Xia, X. Zhang, X. Chu, J. Han, G. Ding, Repmlp: Re-\nparameterizing convolutions into fully-connected layers for image recog-\nnition (2021). arXiv:2105.01883 .\n[30] M.-H. Guo, Z.-N. Liu, T.-J. Mu, S.-M. Hu, Beyond self-attention: Ex-\nternal attention using two linear layers for visual tasks, IEEE Trans-\nactions on Pattern Analysis and Machine Intelligence (2022) 1\u201313 doi:\n10.1109/tpami.2022.3211006 .\nURL http://dx.doi.org/10.1109/TPAMI.2022.3211006\n[31] L. Melas-Kyriazi, Do you even need attention? a stack of feed-forward\nlayers does surprisingly well on imagenet (2021). arXiv:2105.02723 .\n[32] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, B. Guo,\nSwin transformer: Hierarchical vision transformer using shifted windows\n(2021). arXiv:2103.14030 .\n33\n\n--- Page 34 ---\n[33] J. Ho, N. Kalchbrenner, D. Weissenborn, T. Salimans, Axial attention\nin multidimensional transformers (2019). arXiv:1912.12180 .\n[34] T. Luo, Z. Ma, Z.-Q. J. Xu, Y. Zhang, Theory of the frequency princi-\nple for general deep neural networks, arXiv preprint arXiv:1906.09235\n(2019).\n[35] A. Jacot, F. Gabriel, C. Hongler, Neural tangent kernel: Convergence\nand generalization in neural networks, in: Advances in neural informa-\ntion processing systems, 2018, pp. 8571\u20138580.\n[36] T. Luo, Z. Ma, Z.-Q. J. Xu, Y. Zhang, On the exact computation of\nlinear frequency principle dynamics and its generalization, SIAM Jour-\nnal on Mathematics of Data Science 4 (4) (2022) 1272\u20131292. doi:\n10.1137/21m1444400 .\nURL http://dx.doi.org/10.1137/21m1444400\n[37] Y. Zhang, Z.-Q. J. Xu, T. Luo, Z. Ma, Explicitizing an implicit bias\nof the frequency principle in two-layer neural networks (2019). arXiv:\n1905.10264 .\n[38] B. Bordelon, A. Canatar, C. Pehlevan, Spectrum dependent learning\ncurves in kernel regression and wide neural networks (2020). arXiv:\n2002.02561 .\n[39] Y. Cao, Z. Fang, Y. Wu, D.-X. Zhou, Q. Gu, Towards understanding the\nspectral bias of deep learning, arXiv preprint arXiv:1912.01198 (2019).\n[40] R. Basri, D. Jacobs, Y. Kasten, S. Kritchman, The convergence rate\nof neural networks for learned functions of different frequencies (2019).\narXiv:1906.00425 .\n[41] G. Yang, H. Salman, A fine-grained spectral perspective on neural net-\nworks, arXiv preprint arXiv:1907.10599 (2019).\n[42] W. E, C. Ma, L. Wu, Machine learning from a continuous viewpoint,\narXiv preprint arXiv:1912.12777 (2019).\n[43] J. Gan, W. Wang, J. Leng, X. Gao, HiGAN+: Handwriting Imitation\nGAN with Disentangled Representations, ACM Trans. Graphics 42 (1)\n(2022) 1\u201317.\n34\n\n--- Page 35 ---\n[44] V. B. Braginsky, F. Y. Khalili, K. S. Thorne, Quantum Measurement,\nCambridge University Press, 1992.\n[45] K. Jacobs, D. A. Steck, A straightforward introduction to continuous\nquantum measurement, Contemporary Physics 47 (5) (2006) 279\u2013303.\ndoi:10.1080/00107510601101934 .\nURL http://dx.doi.org/10.1080/00107510601101934\n[46] J. H. Lim, J. C. Ye, Geometric GAN, arXiv preprint arXiv:1705.02894\n(2017).\n[47] A.Graves, S.Fern\u00e1ndez, F.Gomez, J.Schmidhuber, Connectionisttem-\nporal classification: labelling unsegmented sequence data with recurrent\nneural networks, in: Proceedings of the 23rd International Conference\non Machine Learning, ICML \u201906, Association for Computing Machin-\nery, New York, NY, USA, 2006, p. 369\u2013376. doi:10.1145/1143844.\n1143891.\nURL https://doi.org/10.1145/1143844.1143891\n[48] X. Chen, Y. Duan, R. Houthooft, J. Schulman, I. Sutskever, P. Abbeel,\nInfogan: Interpretable representation learning by information maximiz-\ning generative adversarial nets (2016). arXiv:1606.03657 .\n[49] J.-Y. Zhu, R. Zhang, D. Pathak, T. Darrell, A. A. Efros, O. Wang,\nE. Shechtman, Toward multimodal image-to-image translation (2017).\narXiv:1711.11586 .\n[50] H.-Y. Lee, H.-Y. Tseng, J.-B. Huang, M. Singh, M.-H. Yang, Di-\nverse Image-to-Image Translation via Disentangled Representations,\nSpringer International Publishing, 2018, p. 36\u201352. doi:10.1007/\n978-3-030-01246-5_3 .\nURL http://dx.doi.org/10.1007/978-3-030-01246-5_3\n[51] D. P. Kingma, J. Ba, Adam: A method for stochastic optimization, in:\nICLR, 2015.\n[52] M. Li, T. Lv, J. Chen, L. Cui, Y. Lu, D. Florencio, C. Zhang, Z. Li,\nF. Wei, Trocr: Transformer-based optical character recognition with\npre-trained models, Proceedings of the AAAI Conference on Artificial\nIntelligence 37 (11) (2023) 13094\u201313102. doi:10.1609/aaai.v37i11.\n35\n\n--- Page 36 ---\n26538.\nURL http://dx.doi.org/10.1609/aaai.v37i11.26538\n36",
  "project_dir": "artifacts/projects/enhanced_cs.LG_2508.21040v1_FW_GAN_Frequency_Driven_Handwriting_Synthesis_wit",
  "communication_dir": "artifacts/projects/enhanced_cs.LG_2508.21040v1_FW_GAN_Frequency_Driven_Handwriting_Synthesis_wit/.agent_comm",
  "assigned_at": "2025-08-29T20:59:55.666694",
  "status": "assigned"
}